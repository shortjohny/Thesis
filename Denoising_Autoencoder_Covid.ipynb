{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.fft\n",
    "import scipy.io\n",
    "from scipy.io import wavfile\n",
    "from scipy.stats import norm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "from sklearn.cluster import KMeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-greece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf(res,b):\n",
    "    (mu, sigma) = norm.fit(res)\n",
    "    distr, bins= np.histogram(res, b,density =True)\n",
    "    y = scipy.stats.norm.pdf( bins, mu, sigma)\n",
    "    return bins,y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_noise(Segments):\n",
    "    #small energy=noisy segments\n",
    "    Energies=np.log(np.sum(Segments**2,axis=1)).reshape(-1,1)\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0).fit(Energies)\n",
    "    labels=kmeans.predict(Energies, sample_weight=None)\n",
    "    c1=Segments[np.where(labels==0)]\n",
    "    c2=Segments[np.where(labels==1)]\n",
    "    if np.mean(Energies[np.where(labels==0)],axis=0)<np.mean(Energies[np.where(labels==1)]):\n",
    "        Small_energy=c1\n",
    "    else:\n",
    "        Small_energy=c2\n",
    "    \n",
    "    Noise=Small_energy[np.where(Small_energy.std(axis=1)<=np.mean(Small_energy.std(axis=1)))]\n",
    "    if Noise.shape[0]>0:\n",
    "        return Noise\n",
    "    else:\n",
    "        return Small_energy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_reduce(sig,sampling_rate,alpha,beta=0.01,time_ms=30,time_s=None):\n",
    "\n",
    "    if time_s==None:\n",
    "        time_s=9999\n",
    "    block_size=int((time_ms*(10**(-3)))*sr)\n",
    "    j=int((np.floor(sig.shape[0]/block_size)))\n",
    "    \n",
    "    Segmented_signal=sig[:j*block_size].reshape(-1,block_size)\n",
    "    \n",
    "    if (np.floor(sig.shape[0]%block_size))!=0:\n",
    "        Lost_signal=sig[j*block_size:]\n",
    "        last_segment=np.zeros(block_size)\n",
    "        last_segment[:Lost_signal.shape[0]]=Lost_signal\n",
    "        last_segment=last_segment.reshape(1,-1)\n",
    "        Segmented_signal=np.concatenate([Segmented_signal,last_segment],axis=0)\n",
    "        \n",
    "    #Find noise\n",
    "    subsignal=sig[:time_s*sampling_rate]\n",
    "    \n",
    "    k=int((np.floor(subsignal.shape[0]/block_size)))\n",
    "    \n",
    "    Segmented_subsignal=subsignal[:k*block_size].reshape(-1,block_size)\n",
    "    \n",
    "    noise=find_noise(Segmented_subsignal)\n",
    "    noise_model=np.mean(np.abs(scipy.fft.fft(noise)),axis=0)\n",
    "    \n",
    "    y_fft=(scipy.fft.fft(Segmented_signal))\n",
    "    Y_mag=(np.abs(y_fft))\n",
    "    Y_angle=np.angle(y_fft)\n",
    "    X_mag=Y_mag-alpha*noise_model\n",
    "    #applying T\n",
    "    if alpha!=1:\n",
    "        X_mag[np.where(X_mag<beta*Y_mag)]=beta*Y_mag[np.where(X_mag<beta*Y_mag)]\n",
    "    X=X_mag*np.exp(1.0j*Y_angle)\n",
    "    x=scipy.fft.ifft(X).real.astype(np.float32)\n",
    "    x=x.reshape(-1)[:sig.shape[0]]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myError(x,y,l,W):\n",
    "  return ((1/2)*(x-y)**2).mean()  +(l/2)*W\n",
    "\n",
    "def kl_divergence(rho, rho_hat):\n",
    "    rho_hat = torch.mean(torch.sigmoid(rho_hat), 1) # sigmoid because we need the bernouli probability distributions\n",
    "    rho = torch.tensor([rho] * (rho_hat).shape[0]).to(device)\n",
    "    return torch.sum(rho * torch.log(rho/rho_hat) + (1 - rho) * torch.log((1 - rho)/(1 - rho_hat)))\n",
    "# define the sparse loss function\n",
    "\n",
    "def sparse_loss(r, input):\n",
    "    leaky=nn.LeakyReLU(0.2)\n",
    "    values = input\n",
    "    loss = 0\n",
    "    model_children = list(model.children())\n",
    "    for i in range(np.array(model_children).shape[0]):\n",
    "      values = leaky((model_children[i](values)))\n",
    "      if i in np.arange(len(model_children))[:-2]:#we need the hidden layers \n",
    "        loss += kl_divergence(r, values)\n",
    "    return loss\n",
    "\n",
    "def calcW(model):\n",
    "  model_children = list(model.children())\n",
    "  loss = 0\n",
    "  for i in range(np.array(model_children).shape[0]):\n",
    "    if i in np.arange(len(model_children))[1:-1]:#proposed\n",
    "        loss += ((abs(model_children[i].weight.data)**2).mean())\n",
    "  return loss\n",
    "\n",
    "def clean_signal(signal,model,window_length,device):\n",
    "  segments=[]\n",
    "  for k in range(int(((signal).shape[0]/window_length))):\n",
    "    segment=signal[k*window_length:(k+1)*window_length]\n",
    "    segments.append(segment)\n",
    "  \n",
    "  if signal.shape[0]%window_length!=0:\n",
    "        last_segment=np.zeros(window_length)\n",
    "        last_segment[:signal[(k+1)*window_length:].shape[0]]=signal[(k+1)*window_length:]\n",
    "        segments.append(last_segment)\n",
    "  segments=torch.tensor(segments,dtype=torch.double).to(device)\n",
    "\n",
    "  clean=model(segments)\n",
    "\n",
    "  \n",
    "    \n",
    "  return clean.reshape(-1).detach().cpu().numpy()[:signal.shape[0]]\n",
    "\n",
    "\n",
    "def choose_model(model_old,model_new,validation_error_old,validation_error_new):\n",
    "    if validation_error_new<validation_error_old or validation_error_new==validation_error_old:\n",
    "        return model_new,validation_error_new\n",
    "    else:\n",
    "        return model_old,validation_error_old"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-external",
   "metadata": {},
   "source": [
    "# Model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals=[]\n",
    "#j='Nikaia'\n",
    "#j='Patra'\n",
    "for j in ['Nikaia']:\n",
    "    for i in range(1,3000):\n",
    "        try:\n",
    "        \n",
    "            _,mixtureR_corr = wavfile.read('/home/myron/Desktop/Diplomatikh/data/F_Covid19/'+j+'_LR_st/'+str(i)+'_R_scaled')\n",
    "            _, mixtureR_clean = wavfile.read('/home/myron/Desktop/Diplomatikh/data/F_Covid19/'+j+'_LR_dn/'+str(i)+'_R_denoised')\n",
    "        \n",
    "            _, mixtureL_corr = wavfile.read('/home/myron/Desktop/Diplomatikh/data/F_Covid19/'+j+'_LR_st/'+str(i)+'_L_scaled')\n",
    "            _, mixtureL_clean = wavfile.read('/home/myron/Desktop/Diplomatikh/data/F_Covid19/'+j+'_LR_dn/'+str(i)+'_L_denoised')\n",
    "            signals.append([mixtureR_corr,mixtureR_clean])\n",
    "            signals.append([mixtureL_corr,mixtureL_clean])\n",
    "       \n",
    "        except:\n",
    "            continue\n",
    "signals=np.array(signals,dtype=object)\n",
    "\n",
    "#segment the signals into windows (non overlapping)\n",
    "segments=[]\n",
    "\n",
    "window_length=784\n",
    "for corr,clean in signals:\n",
    "  for k in range(int(((corr).shape[0]/window_length))):\n",
    "    \n",
    "    segment_corr=corr[k*window_length:(k+1)*window_length]\n",
    "    segment_clean=clean[k*window_length:(k+1)*window_length]\n",
    "    \n",
    "    segments.append([segment_corr,segment_clean])\n",
    "segments=np.array(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(segments)\n",
    "\n",
    "f=int(segments.shape[0]*0.8)\n",
    "#split the data to trainset/testset\n",
    "trainset= segments[:f]\n",
    "testset= segments[f:]\n",
    "\n",
    "input_size=window_length\n",
    "batch_size=64\n",
    "\n",
    "#loading the data into tensors \n",
    "X_train=torch.tensor(trainset,dtype=torch.double)\n",
    "X_test=torch.tensor(testset,dtype=torch.double)\n",
    "dl_train=DataLoader(X_train,batch_size,shuffle=True)\n",
    "dl_test=DataLoader(X_test,batch_size,shuffle=True)\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    #Works Best\n",
    "    #2 Layers Double size\n",
    "    def __init__(self):\n",
    "        super( AutoEncoder, self).__init__()\n",
    "        # encoder\n",
    "        self.enc1 = nn.Linear(input_size,768,bias=True)\n",
    "        self.enc2 = nn.Linear(in_features=768, out_features=700,bias=True)\n",
    "        \n",
    "        # decoder \n",
    "        \n",
    "        self.dec1 = nn.Linear(in_features=700, out_features=768,bias=True)\n",
    "        self.dec2 = nn.Linear(in_features=768, out_features=input_size,bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #paper proposes sigmoid \n",
    "        self.leaky=nn.LeakyReLU(0.2)\n",
    "        # encoding\n",
    "        x = self.leaky(self.enc1(x))\n",
    "        x = self.leaky(self.enc2(x))\n",
    "        \n",
    "        # decoding\n",
    "        x = self.leaky(self.dec1(x))\n",
    "        x = (self.dec2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "learning_rate=1e-5\n",
    "model=AutoEncoder().double().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "Error=[]\n",
    "Validation=[]\n",
    "Outputs=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs=900\n",
    "lamda=1\n",
    "beta=0\n",
    "rho=0.05\n",
    "#when to switch?\n",
    "delta=1e-1\n",
    "\n",
    "for epoch in range(Epochs):\n",
    "  e=[]\n",
    "  for i in dl_train:\n",
    "    corrupted_data,clean_data=i[:,0,:],i[:,1,:]\n",
    "    corrupted_data=corrupted_data.reshape(-1,input_size).to(device)\n",
    "    clean_data=clean_data.reshape(-1,input_size).to(device)\n",
    "\n",
    "    #\n",
    "    s=clean_data.cpu().numpy()\n",
    "    clean_data[np.where(s.std(axis=1)<delta)]=torch.zeros(clean_data[np.where(s.std(axis=1)<delta)].shape,dtype=torch.float64).to(device)\n",
    "    \n",
    "        # ===================forward=====================\n",
    "       \n",
    "    output = model(corrupted_data)\n",
    "    W=calcW(model).to(device)\n",
    "    loss = myError(output,clean_data,lamda,W) \n",
    "    #+ beta*sparse_loss(rho, corrupted_data).to(device)\n",
    "        # ===================backward====================\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    e.append(loss.item())\n",
    "    with torch.no_grad():\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "  with torch.no_grad():\n",
    "      v=[] #v contains the validation loss of all testset batches\n",
    "      model_children=list(model.children())\n",
    "      for j in dl_test:\n",
    "        corrupted_data_test,clean_data_test=j[:,0,:],j[:,1,:]\n",
    "        corrupted_data_test=corrupted_data_test.reshape(-1,input_size).to(device)\n",
    "        clean_data_test=clean_data_test.reshape(-1,input_size).to(device)\n",
    "        \n",
    "        \n",
    "        s=clean_data_test.cpu().numpy()\n",
    "        clean_data_test[np.where(s.std(axis=1)<delta)]=torch.zeros(clean_data_test[np.where(s.std(axis=1)<delta)].shape,dtype=torch.float64).to(device)\n",
    "        \n",
    "        output_test = model(corrupted_data_test)\n",
    "        W=calcW(model).to(device)\n",
    "        \n",
    "        validation = myError(output_test,clean_data_test,lamda,W) \n",
    "        #+beta*sparse_loss(rho, corrupted_data_test).to(device)\n",
    "        v.append(validation.item())\n",
    "  \n",
    "\n",
    "  if epoch==0:\n",
    "        model_old=model\n",
    "        validation_old=np.array(v).mean()\n",
    "        validation_new=np.array(v).mean()\n",
    "        model,validation_old=choose_model(model_old,model,validation_old,validation_new)\n",
    "        model_old=model\n",
    "        \n",
    "        Error.append(np.array(e).mean())\n",
    "        Validation.append(validation_old)\n",
    "  else:\n",
    "    validation_new=np.array(v).mean()\n",
    "    model,validation_old=choose_model(model_old,model,validation_old,validation_new)\n",
    "    model_old=model\n",
    "    \n",
    "    Error.append(np.array(e).mean())\n",
    "    Validation.append(validation_old)\n",
    "    \n",
    "    \n",
    "  print('epoch [{}/{}], loss:{:.6f} , validation loss:{:.6f}'\n",
    "          .format(epoch + 1, Epochs, np.array(e).mean(),validation_old) )\n",
    "\n",
    "\n",
    "torch.save(model,'AutoencoderNikaia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-faith",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Errors')\n",
    "plt.plot(Error,label = \"Training error\")\n",
    "plt.plot(Validation,label = \"Validation error\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#plt.plot(Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Training error\")\n",
    "plt.plot(Error)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Validation error\")\n",
    "plt.plot(Validation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-chassis",
   "metadata": {},
   "source": [
    "# Denoising Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPatra=torch.load('AutoencoderPatra')\n",
    "modelNikaia=torch.load('AutoencoderNikaia')\n",
    "model=torch.load('AutoencoderWhole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-mailman",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "j='Nikaia'\n",
    "#j='Patra'\n",
    "for i in range(1,3000):\n",
    "    try:\n",
    "        samplerate, data = wavfile.read('/home/myron/Desktop/Diplomatikh/data/F_Covid19/'+j+'_LR_st/'+str(i)+'_R_scaled')\n",
    "        \n",
    "        out=clean_signal(data.reshape(-1),modelNikaia,window_length,device)\n",
    "        \n",
    "        k=min([data.shape[0],out.shape[0]])\n",
    "        \n",
    "        \n",
    "        res=data[:k]-out[:k]\n",
    "        \n",
    "\n",
    "        (mu, sigma) = norm.fit(res)\n",
    "        plt.title(str(i))\n",
    "        n, bins, patches = plt.hist(res, 300, facecolor='green',density =True,stacked =True)\n",
    "\n",
    "        y = scipy.stats.norm.pdf( bins, mu, sigma)\n",
    "        \n",
    "        l = plt.plot(bins, y, linewidth=2)\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "j='Nikaia'\n",
    "#j='Patra'\n",
    "for i in range(2000,3000):\n",
    "    try:\n",
    "        samplerate, data = wavfile.read('/home/myron/Desktop/Diplomatikh/data/F_Covid19/'+j+'_LR_st/'+str(i)+'_R_scaled')\n",
    "        \n",
    "        out=clean_signal(data.reshape(-1),modelNikaia,window_length,device)\n",
    "        \n",
    "        k=min([data.shape[0],out.shape[0]])\n",
    "        \n",
    "        #plt.plot(data)\n",
    "        #plt.plot(out)\n",
    "        res=data[:k]-out[:k]\n",
    "        plt.plot(res)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-reality",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-manchester",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiments with extented spectral gating\n",
    "\n",
    "j='Patra'\n",
    "i=43\n",
    "\n",
    "sr, mixtureR1 = wavfile.read('/home/myron/Desktop/Diplomatikh/data/F_Covid19/'+j+'_LR_st/'+str(i)+'_R_scaled')\n",
    "sr, mixtureL1 = wavfile.read('/home/myron/Desktop/Diplomatikh/data/F_Covid19/'+j+'_LR_st/'+str(i)+'_L_scaled')\n",
    "\n",
    "s_R1=clean_signal(mixtureR1.reshape(-1),modelPatra,window_length,device)\n",
    "s_L1=clean_signal(mixtureL1.reshape(-1),modelPatra,window_length,device)\n",
    "\n",
    "\n",
    "wavfile.write(str(i)+'_R_AEdenoised', sr, s_R1)\n",
    "wavfile.write(str(i)+'_L_AEdenoised', sr, s_L1)\n",
    "wavfile.write(str(i)+'_L_AEnoise', sr, mixtureL1-s_L1)\n",
    "wavfile.write(str(i)+'_R_AEnoise', sr, mixtureR1-s_R1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "j='Nikaia'\n",
    "i=2457\n",
    "\n",
    "sr, mixtureR2 = wavfile.read('/home/myron/Desktop/Diplomatikh/data/F_Covid19/'+j+'_LR_st/'+str(i)+'_R_scaled')\n",
    "sr, mixtureL2 = wavfile.read('/home/myron/Desktop/Diplomatikh/data/F_Covid19/'+j+'_LR_st/'+str(i)+'_L_scaled')\n",
    "s_R2=clean_signal(mixtureR2.reshape(-1),modelNikaia,window_length,device)\n",
    "s_L2=clean_signal(mixtureL2.reshape(-1),modelNikaia,window_length,device)\n",
    "\n",
    "\n",
    "wavfile.write(str(i)+'_R_AEdenoised', sr, s_R2)\n",
    "wavfile.write(str(i)+'_L_AEdenoised', sr, s_L2)\n",
    "wavfile.write(str(i)+'_L_AEnoise', sr, mixtureL2-s_L2)\n",
    "wavfile.write(str(i)+'_R_AEnoise', sr, mixtureR2-s_R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-married",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization of  extented spectral denoising\n",
    "data=mixtureL1\n",
    "out=s_L1\n",
    "name='Patra:43'\n",
    "\n",
    "gridsize = (3, 3)\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "fig.suptitle(name,fontsize=16)\n",
    "ax1 = plt.subplot2grid(gridsize, (0, 0))\n",
    "ax2 = plt.subplot2grid(gridsize, (0, 1))\n",
    "ax3 = plt.subplot2grid(gridsize, (0, 2))\n",
    "\n",
    "ax4 = plt.subplot2grid(gridsize, (1, 0))\n",
    "ax5 = plt.subplot2grid(gridsize, (1, 1))\n",
    "ax6 = plt.subplot2grid(gridsize, (1, 2))\n",
    "\n",
    "\n",
    "ax1.set_title('Noisy signal:')\n",
    "ax1.plot(data)\n",
    "ax2.set_title('Clean signal:')\n",
    "ax2.plot(out)\n",
    "ax3.set_title('Residual:')\n",
    "ax3.plot((data-out))\n",
    "\n",
    "\n",
    "ax4.specgram(data,NFFT=1024,Fs=48000,noverlap=512)\n",
    "ax5.specgram(out,NFFT=1024,Fs=48000,noverlap=512)\n",
    "ax6.specgram(data-out,NFFT=1024,Fs=48000,noverlap=512)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-output",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=mixtureL2\n",
    "out=s_L2\n",
    "name='Nikaia:2457'\n",
    "\n",
    "gridsize = (3, 3)\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "fig.suptitle(name,fontsize=16)\n",
    "ax1 = plt.subplot2grid(gridsize, (0, 0))\n",
    "ax2 = plt.subplot2grid(gridsize, (0, 1))\n",
    "ax3 = plt.subplot2grid(gridsize, (0, 2))\n",
    "\n",
    "ax4 = plt.subplot2grid(gridsize, (1, 0))\n",
    "ax5 = plt.subplot2grid(gridsize, (1, 1))\n",
    "ax6 = plt.subplot2grid(gridsize, (1, 2))\n",
    "\n",
    "\n",
    "ax1.set_title('Noisy signal:')\n",
    "ax1.plot(data)\n",
    "ax2.set_title('Clean signal:')\n",
    "ax2.plot(out)\n",
    "ax3.set_title('Residual:')\n",
    "ax3.plot((data-out))\n",
    "\n",
    "\n",
    "ax4.specgram(data,NFFT=1024,Fs=48000,noverlap=512)\n",
    "ax5.specgram(out,NFFT=1024,Fs=48000,noverlap=512)\n",
    "ax6.specgram(data-out,NFFT=1024,Fs=48000,noverlap=512)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-spice",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noise distribution\n",
    "noisePatra=mixtureL1-s_L1\n",
    "noiseNikaia=mixtureL2-s_L2\n",
    "\n",
    "fig, (ax5,ax6)= plt.subplots(2,1,constrained_layout = True)\n",
    "\n",
    "ax5.title.set_text('Patra noise distribution:'+str(43))\n",
    "ax6.title.set_text('Nikaia noise distribution:'+str(2457))\n",
    "\n",
    "ax5.hist( noisePatra,bins=200,range=[-.4,.4])\n",
    "ax6.hist( noiseNikaia,bins=200,range=[-1.5,1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noise distributions\n",
    "\n",
    "s_R4=noise_reduce(mixtureR1,sr,1,beta=0.01,time_ms=1)\n",
    "s_L4=noise_reduce(mixtureL1,sr,1,beta=0.01,time_ms=1)\n",
    "\n",
    "\n",
    "s_R5=noise_reduce(mixtureR2,sr,1,beta=0.01,time_ms=1)\n",
    "s_L5=noise_reduce(mixtureL2,sr,1,beta=0.01,time_ms=1)\n",
    "\n",
    "\n",
    "gridsize = (4, 4)\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "\n",
    "ax1 = plt.subplot2grid(gridsize, (0, 0))\n",
    "ax2 = plt.subplot2grid(gridsize, (0, 1))\n",
    "ax3 = plt.subplot2grid(gridsize, (0, 2))\n",
    "ax4 = plt.subplot2grid(gridsize, (0, 3))\n",
    "\n",
    "\n",
    "\n",
    "ax5 = plt.subplot2grid(gridsize, (1, 0))\n",
    "ax6 = plt.subplot2grid(gridsize, (1, 1))\n",
    "ax7 = plt.subplot2grid(gridsize, (1, 2))\n",
    "ax8 = plt.subplot2grid(gridsize, (1, 3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax1.set_title('Spectral Subtraction noise:43')\n",
    "ax1.plot((s_L4-mixtureL1))\n",
    "\n",
    "ax2.set_title('Denoising Autoencoder noise:43')\n",
    "ax2.plot((s_L1-mixtureL1))\n",
    "\n",
    "ax3.set_title('Spectral Subtraction noise:2457')\n",
    "ax3.plot((s_L5-mixtureL2))\n",
    "ax3.set_xticks(np.arange(0, s_L5.shape[0], step=200000))\n",
    "ax4.set_title('Denoising Autoencoder noise:2457')\n",
    "ax4.plot((s_L2-mixtureL2))\n",
    "ax4.set_xticks(np.arange(0, s_L2.shape[0], step=200000))\n",
    "\n",
    "ax5.set_title('Noise distribution:')\n",
    "ax5.hist((s_L4-mixtureL1),200,range=[-0.4,0.4],density=True)\n",
    "bins5,y5=pdf(s_L4-mixtureL1,200)\n",
    "ax5.plot(bins5, y5, linewidth=2)\n",
    "\n",
    "ax6.set_title('Noise distribution:')\n",
    "ax6.hist((s_L1-mixtureL1),200,range=[-0.4,0.4],density=True)\n",
    "bins6,y6=pdf((s_L1-mixtureL1),200)\n",
    "ax6.set_xlim([-0.4,0.4])\n",
    "ax6.plot(bins6, y6, linewidth=2)\n",
    "\n",
    "ax7.set_title('Noise distribution:')\n",
    "ax7.hist((s_L5-mixtureL2),200,range=[-1.,1.],density=True)\n",
    "bins7,y7=pdf((s_L5-mixtureL2),200)\n",
    "ax7.plot(bins7, y7, linewidth=2)\n",
    "ax7.set_xlim([-1.,1.])\n",
    "\n",
    "\n",
    "ax8.set_title('Noise distribution:')\n",
    "ax8.hist((s_L2-mixtureL2),200,range=[-1.,1.],density=True)\n",
    "bins8,y8=pdf((s_L2-mixtureL2),200)\n",
    "ax8.plot(bins8, y8, linewidth=2)\n",
    "ax8.set_xlim([-1.,1.])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-colors",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Signals with powerfull noise.\n",
    "j='Patra'\n",
    "i=64\n",
    "\n",
    "\n",
    "sr, mixtureR3 = wavfile.read('/home/myron/Desktop/Diplomatikh/data/F_Covid19/'+j+'_LR_st/'+str(i)+'_R_scaled')\n",
    "sr, mixtureL3 = wavfile.read('/home/myron/Desktop/Diplomatikh/data/F_Covid19/'+j+'_LR_st/'+str(i)+'_L_scaled')\n",
    "\n",
    "s_R3=clean_signal(mixtureR3.reshape(-1),modelPatra,window_length,device)\n",
    "s_L3=clean_signal(mixtureL3.reshape(-1),modelPatra,window_length,device)\n",
    "\n",
    "wavfile.write(str(i)+'_R_AEdenoised', sr, s_R3)\n",
    "wavfile.write(str(i)+'_L_AEdenoised', sr, s_L3)\n",
    "wavfile.write(str(i)+'_L_AEnoise', sr, mixtureL3-s_L3)\n",
    "wavfile.write(str(i)+'_R_AEnoise', sr, mixtureR3-s_R3)\n",
    "\n",
    "#visualization of denoising\n",
    "#------------------------------------------------------------------------\n",
    "data=mixtureL3\n",
    "out=s_L3\n",
    "name='Patra:64'\n",
    "\n",
    "gridsize = (3, 3)\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "fig.suptitle(name,fontsize=16)\n",
    "ax1 = plt.subplot2grid(gridsize, (0, 0))\n",
    "ax2 = plt.subplot2grid(gridsize, (0, 1))\n",
    "ax3 = plt.subplot2grid(gridsize, (0, 2))\n",
    "\n",
    "ax4 = plt.subplot2grid(gridsize, (1, 0))\n",
    "ax5 = plt.subplot2grid(gridsize, (1, 1))\n",
    "ax6 = plt.subplot2grid(gridsize, (1, 2))\n",
    "\n",
    "\n",
    "ax1.set_title('Noisy signal:')\n",
    "ax1.plot(data)\n",
    "ax2.set_title('Clean signal:')\n",
    "ax2.plot(out)\n",
    "ax3.set_title('Residual:')\n",
    "ax3.plot((data-out))\n",
    "\n",
    "\n",
    "ax4.specgram(data,NFFT=1024,Fs=48000,noverlap=512)\n",
    "ax5.specgram(out,NFFT=1024,Fs=48000,noverlap=512)\n",
    "ax6.specgram(data-out,NFFT=1024,Fs=48000,noverlap=512)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-adams",
   "metadata": {},
   "source": [
    "# Data denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "j='Patra'\n",
    "\n",
    "modelPatra=torch.load('AutoencoderPatra')\n",
    "\n",
    "for i in range(1,82):\n",
    "    try:\n",
    "        samplerate, data_R = wavfile.read('/home/myron/Desktop/Diplomatikh/data/F_Covid19/'+j+'_LR_st/'+str(i)+'_R_scaled')\n",
    "        samplerate, data_L = wavfile.read('/home/myron/Desktop/Diplomatikh/data/F_Covid19/'+j+'_LR_st/'+str(i)+'_L_scaled')\n",
    "        out_R=clean_signal(data_R.reshape(-1),modelPatra,window_length,device)\n",
    "        out_L=clean_signal(data_L.reshape(-1),modelPatra,window_length,device)\n",
    "        wavfile.write(str(i)+'_pydeNoised_R.wav', samplerate, out_R)\n",
    "        wavfile.write(str(i)+'_pydeNoised_L.wav', samplerate, out_L)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "j='Nikaia'\n",
    "\n",
    "modelNikaia=torch.load('AutoencoderNikaia')\n",
    "\n",
    "for i in range(1000,3000):\n",
    "    try:\n",
    "        samplerate, data_R = wavfile.read('/home/myron/Desktop/Diplomatikh/data/F_Covid19/'+j+'_LR_st/'+str(i)+'_R_scaled')\n",
    "        samplerate, data_L = wavfile.read('/home/myron/Desktop/Diplomatikh/data/F_Covid19/'+j+'_LR_st/'+str(i)+'_L_scaled')\n",
    "        out_R=clean_signal(data_R.reshape(-1),modelNikaia,window_length,device)\n",
    "        out_L=clean_signal(data_L.reshape(-1),modelNikaia,window_length,device)\n",
    "        wavfile.write(str(i)+'_pydeNoised_R.wav', samplerate, out_R)\n",
    "        wavfile.write(str(i)+'_pydeNoised_L.wav', samplerate, out_L)\n",
    "    except:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
